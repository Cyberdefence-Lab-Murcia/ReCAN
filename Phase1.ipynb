{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automotive Data analyser\n",
    "\n",
    "- Phase 1: CSV files with the format `ABSOLUTE_TIME, ID, BINARY`\n",
    "- Phase 2: CSV files with the format `ABSOLUTE_TIME, can_id_var, can_id_var, ...` where `can = {0,1}`, `id` is a unique identifier for the source and `var` is the identifier of the decoded field.\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Initial setup](#Initial-setup)\n",
    "- [Preprocessing](#Preprocessing)\n",
    "  - [Load binary data frames](#Load-binary-data-frames)\n",
    "  - [Calculate interarrival packet times for each ID and CANline](#Calculate-interarrival-packet-times-for-each-ID-and-CANline)\n",
    "  - [Calculate bitflips and bitflips magnitude](#Calculate-bitflips-and-bitflips-magnitude)\n",
    "- [Data field decode](#Data-field-decode)\n",
    "  - [Data statistics](#Data-statistics)\n",
    "  - [Datablock Analysis](#Datablocks-analysis)\n",
    "- [Export figures](#Export-figures)\n",
    "  - [Packet count per ID and CANline](#Packet-count-per-ID-and-CANline)\n",
    "  - [Interrarrival packet time distributions and boxplots](#Interrarrival-packet-time-distributions-and-boxplots)\n",
    "  - [Bitflips heatmaps](#Bitflips-heatmaps)\n",
    "  \n",
    "Once the preprocess is complete, you may skip directly to the decode section to speed-up the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import statistics \n",
    "import math\n",
    "import pickle\n",
    "import scipy.stats as scstat            \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# If true exports vectorial PDFs instead of JPG.\n",
    "VECTORIAL_FIGURES = False\n",
    "FIG_EXTENSION = \"pdf\" if VECTORIAL_FIGURES else \"jpg\"\n",
    "\n",
    "ROOT_DIR = \"absolute-path-to-project-root-folder\"\n",
    "VEHICLE = \"vechile/experiment\"\n",
    "DATA_DIR = ROOT_DIR + \"Data/\" + VEHICLE + \"/\"\n",
    "GRAPHICS_DIR = ROOT_DIR + \"Graphics/\" + VEHICLE + \"/\" + FIG_EXTENSION + \"/\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(GRAPHICS_DIR)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    "\n",
    "# Either use pandas or modin, not both\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# Configure RAY and MODIN\n",
    "#import modin.pandas as mpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Load binary data frames\n",
    "Import the raw file, rename the columns accordingly and parse the timestamp as index.\n",
    "This step can be skipped if the dataframe has been already processed and stored as indicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load the CSV\n",
    "df = pd.read_csv(DATA_DIR + \"raw.csv\", \n",
    "                 #sep= r' +|,|\\t', # Files may have one or more spaces as separator (also include commas and tabs).\n",
    "                 sep=\",\",\n",
    "                 dtype={0:object, 1:object, 2:object, 3:object, 4:object}, \n",
    "                 #nrows=1000,\n",
    "                 header=None\n",
    "                )\n",
    "\n",
    "df.rename(columns={0:'time', 1:'CAN', 2:'id', 3:'bytes', 4:'binary'}, inplace=True)\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "\n",
    "# Convert timestamp column to datetime index\n",
    "timestamps = np.around(df['time'].astype(float), 6).apply(datetime.fromtimestamp)\n",
    "datetimes = pd.to_datetime(timestamps)\n",
    "idx = pd.DatetimeIndex(datetimes, freq='infer').copy(deep=True)\n",
    "#df.set_index(idx, inplace=True)\n",
    "#df.drop('time', axis=1, inplace=True)\n",
    "df['time'] = idx\n",
    "\n",
    "# Remove support variables\n",
    "del(timestamps)\n",
    "del(datetimes)\n",
    "del(idx)\n",
    "\n",
    "# Convert CAN line and ID columns to categorical data\n",
    "df['can'] = pd.Categorical(df['can'])\n",
    "df['id'] = pd.Categorical(df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the intermediate result as Pickle file to speedup future analysis\n",
    "#df.to_pickle(DATA_DIR + \"raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "display(df.can.unique())\n",
    "display(df.id.unique())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate interarrival packet times for each ID and CANline\n",
    "\n",
    "For each uid in the RAW data, extract the interarrival packet time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df.loc[:,'timedelta'] = np.nan\n",
    "df.loc[:,'timedelta_ms'] = np.nan\n",
    "\n",
    "grp = df.groupby(by=['id','can'])\n",
    "for (uid, can), group in progress_bar(grp):     \n",
    "    try:\n",
    "        diffs = group.time.diff().dropna()\n",
    "        diffs_ms = np.multiply(diffs.dt.total_seconds(),1000)\n",
    "\n",
    "        df.loc[diffs.index, 'timedelta'] = diffs\n",
    "        df.loc[diffs_ms.index, 'timedelta_ms'] = diffs_ms\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"EXCEPTION!!\", uid, can, ex)\n",
    "        display(group)\n",
    "        break\n",
    "\n",
    "\n",
    "# Drop first element of each (CAN,ID) pair which will be NaT anyway\n",
    "#data = data[np.isfinite(data['timedelta_ms'])]\n",
    "df.dropna(subset=['timedelta_ms'], inplace=True)\n",
    "\n",
    "# Add a log transform to the ms field\n",
    "df.loc[:,'timedelta_ms_log'] = np.log10(df['timedelta_ms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "display(df.can.unique())\n",
    "display(df.id.unique())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the intermediate result as Pickle file to speedup future analysis\n",
    "#df.to_pickle(DATA_DIR + \"raw.interrarrivaltimes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate bitflips and bitflips magnitude\n",
    "\n",
    "Calculate the bitflips for each bit in the frame, grouped by ID and CANline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def create_bitflips(data):    \n",
    "    import math\n",
    "    \n",
    "    bitflips = pd.DataFrame()\n",
    "    bitflips['id'] = \"\"\n",
    "    bitflips['can'] = \"\"\n",
    "    \n",
    "    magnitudes = pd.DataFrame()\n",
    "    magnitudes['id'] = \"\"\n",
    "    magnitudes['can'] = \"\"\n",
    "\n",
    "    gprs = data.groupby(by=['id','can'])\n",
    "    mb = master_bar(gprs)\n",
    "\n",
    "    for (uid, can), group in mb:   \n",
    "        binary_string_length = group.binary.str.len().max()\n",
    "        batch_size = len(group)\n",
    "\n",
    "        pb = progress_bar(group.binary, parent=mb)\n",
    "        pb.comment = \"Converting bits string to bits array\"\n",
    "        chrs = [list(c) for c in pb]\n",
    "\n",
    "        pb = progress_bar(range(len(chrs)), parent=mb)\n",
    "        pb.comment = \"Converting bits array to numeric\"\n",
    "        for i in pb:\n",
    "            chrs[i] = pd.to_numeric(chrs[i])\n",
    "\n",
    "        tmp_dict = {\n",
    "            \"id\": uid,\n",
    "            \"can\": can\n",
    "        }\n",
    "        tmp_dict_mag = {\n",
    "            \"id\": uid,\n",
    "            \"can\": can\n",
    "        }\n",
    "\n",
    "        #print(\"vstack\")\n",
    "        tmp = np.vstack(chrs)\n",
    "        #display(tmp[1:5,:])\n",
    "\n",
    "        pb = progress_bar(range(binary_string_length), parent=mb)\n",
    "        pb.comment = \"Calculating bitflips\"\n",
    "        for c in pb:\n",
    "            tmpC = tmp[:,c]\n",
    "            #print(''.join(map(str, tmpC)))\n",
    "            tmpS = np.sum(np.abs(np.diff(tmpC)))\n",
    "            #print(\"Sum\", tmpS)\n",
    "            tmp_dict[c] = tmpS / batch_size\n",
    "            try:\n",
    "                # Beware that in READ (Marchetti2019) the function is math.ceiling not math.floor\n",
    "                # However, as described in our article, this is not affecting the results.\n",
    "                tmp_dict_mag[c] = int(math.floor(math.log10(tmpS / batch_size)))\n",
    "            except Exception as ex:\n",
    "                tmp_dict_mag[c] = float('-inf')\n",
    "\n",
    "        #print(uid, can, batch_size, tmp.shape, ''.join(map(str, tmp_dict.values())))\n",
    "        \n",
    "        bitflips = bitflips.append(tmp_dict, ignore_index=True)\n",
    "        magnitudes = magnitudes.append(tmp_dict_mag, ignore_index=True)\n",
    "\n",
    "    return bitflips, magnitudes\n",
    "\n",
    "bitflips, magnitudes = create_bitflips(df)#.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(bitflips.info())\n",
    "display(bitflips.head())\n",
    "#display(magnitudes.info())\n",
    "display(magnitudes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the intermediate result as Pickle file to speedup future analysis\n",
    "#bitflips.to_pickle(DATA_DIR + \"raw.bitflips.pkl\")\n",
    "#magnitudes.to_pickle(DATA_DIR + \"raw.magnitudes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data field decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should always load the binary extended with the interarrival packet time\n",
    "#df = pd.read_pickle(DATA_DIR + \"raw.pkl\")\n",
    "#df = pd.read_pickle(DATA_DIR + \"raw.interrarrivaltimes.pkl\")\n",
    "\n",
    "# Load the PKL files instead of re-executing the analysis.\n",
    "#bitflips = pd.read_pickle(DATA_DIR + \"raw.bitflips.pkl\")\n",
    "#magnitudes = pd.read_pickle(DATA_DIR + \"raw.magnitudes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(2))\n",
    "display(bitflips.head(2))\n",
    "display(magnitudes.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the lower and upper bounds of the time series. It will be used for graphical purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xlim_min = min(df.time)\n",
    "xlim_max = max(df.time)\n",
    "print(\"Time limits:\", xlim_min, xlim_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datablocks analysis\n",
    "\n",
    "This analysis converts the binary data frames into a unified data format where there is a value for each instant, ID, CANline and variable trace.\n",
    "\n",
    "#### Identify the datablocks boundaries\n",
    "It cycles over the dataframe `bitflips` that presents a row for each `ID` and `CANline`. The algorithm implements the Phase1 algorithm of [Marchetti2017](http://dx.doi.org/10.1109/TIFS.2018.2870826).\n",
    "\n",
    "The intended output is a dictionary of datablocks to be analysed (`raw_datablocks`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_datablocks = {}\n",
    "\n",
    "# Cicle over the dataframe that contains the bitflips.\n",
    "for idx, row in bitflips.iterrows():\n",
    "    # Bitflips list, remove both ID and CANline\n",
    "    b = list(bitflips.loc[idx,:])\n",
    "    uid = b[0]\n",
    "    can = b[1]\n",
    "    del b[0]\n",
    "    del b[0]\n",
    "    \n",
    "    try:\n",
    "        limit = list(map(tuple, np.where(np.isnan(b))))[0][0]\n",
    "    except:\n",
    "        limit = len(b)\n",
    "        \n",
    "    # If there are no bitflips in the dataset, provide a placeholder value\n",
    "    if not np.asarray(b).any():\n",
    "        raw_datablocks[(uid, can)] = None\n",
    "        continue\n",
    "    \n",
    "    # Datablocks extracted from the current ID-CANline\n",
    "    current_datablocks = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    end_idx = None\n",
    "    while start_idx < limit:\n",
    "        #print(\"Main\", uid, can, start_idx, end_idx)\n",
    "        while start_idx < limit and b[start_idx] == 0:\n",
    "            start_idx += 1\n",
    "            #print(\" - SKIP START\", start_idx, end_idx)\n",
    "        end_idx = start_idx\n",
    "        while end_idx < limit:\n",
    "            #print(\" - CHECK END \", start_idx, end_idx)\n",
    "            if not np.isfinite(b[end_idx]):\n",
    "                #print(\" - - End Seq.\\t(\", start_idx, \",\", end_idx-1, \")\")\n",
    "                current_datablocks.append({'start_idx': start_idx, 'end_idx': end_idx -1})\n",
    "                end_idx = limit\n",
    "                start_idx = limit\n",
    "                break\n",
    "            elif b[end_idx] == 0:\n",
    "                #print(\" - - B is 0\\t(\", start_idx, \",\", end_idx-1, \")\")\n",
    "                current_datablocks.append({'start_idx': start_idx, 'end_idx': end_idx -1})\n",
    "                start_idx = end_idx -1\n",
    "                break\n",
    "            elif end_idx == limit -1:\n",
    "                #print(\" - - End Seq.\\t(\", start_idx, \",\", end_idx, \")\")\n",
    "                current_datablocks.append({'start_idx': start_idx, 'end_idx': end_idx})\n",
    "                start_idx = end_idx\n",
    "                break\n",
    "            \n",
    "            end_idx += 1\n",
    "        start_idx += 1\n",
    "    raw_datablocks[(uid, can)] = current_datablocks\n",
    "    print(uid, can, current_datablocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the intermediate result as Pickle file to speedup future analysis\n",
    "#with open(DATA_DIR + 'raw.rawdatablocks.pkl', 'wb') as handle:\n",
    "#    pickle.dump(raw_datablocks, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognize data types\n",
    "It cycles over the datablocks identified in the previous phase (`raw_datablocks`), the bitflips and the magnitudes. \n",
    "\n",
    "The algorithm implements the Phase2 algorithm of [Marchetti2017](http://dx.doi.org/10.1109/TIFS.2018.2870826).\n",
    "\n",
    "Use the flag `VERBOSE` to enable extra logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "\n",
    "# Create the UID,CANline groups\n",
    "bitflips_grp = bitflips.groupby(by=['id', 'can'])\n",
    "magnitudes_grp = magnitudes.groupby(by=['id', 'can'])\n",
    "\n",
    "# Final dictionary with all variables\n",
    "datablocks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(DATA_DIR + 'raw.rawdatablocks.pkl', 'rb') as handle:\n",
    "#    raw_datablocks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function separate the raw datablocks according to the magnitude heuristic ([Marchetti2017](http://dx.doi.org/10.1109/TIFS.2018.2870826)).\n",
    "\n",
    "The intended return of the method consists of two variables:\n",
    " - the `processed_datablocks` variable that includes the final variable split.\n",
    " - the `remaining_blocks` collection that includes the datablocks that still require further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_datablocks(uid, can, raw_datablocks, b, m):\n",
    "    current_raw_datablocks = raw_datablocks\n",
    "    \n",
    "    processed_datablocks = {\n",
    "        'binary': [],\n",
    "        'counter': [],\n",
    "        'crc': [],\n",
    "        'nibble': [],\n",
    "        'byte': [],\n",
    "        'halfword': [],\n",
    "        'word': [],\n",
    "    }\n",
    "    \n",
    "    # Skip the ID-CANline if there are no datablocks\n",
    "    # a.k.a all the bits in all the frames are constant.\n",
    "    if not current_raw_datablocks:\n",
    "        if VERBOSE:\n",
    "            print(\"\\t  SKIP\")\n",
    "        return processed_datablocks, []\n",
    "        \n",
    "    # Blocks still to be analysed\n",
    "    remaining_blocks = []\n",
    "    \n",
    "    # For each datablock\n",
    "    for datablock in current_raw_datablocks:\n",
    "        if VERBOSE:\n",
    "            print(\"\\t\" + str(datablock))\n",
    "            \n",
    "        # Binary datablocks are easy to separate from the others\n",
    "        if datablock['start_idx'] == datablock['end_idx']:\n",
    "            if VERBOSE:\n",
    "                print(\"\\t  BINARY\", datablock)\n",
    "            processed_datablocks['binary'].append(datablock)\n",
    "            continue\n",
    "        \n",
    "        # If matching the conditions split the datablock\n",
    "        start = datablock['start_idx']\n",
    "        for end in range(datablock['start_idx'], datablock['end_idx'] + 1):\n",
    "            \n",
    "            #DEBUG ONLY\n",
    "            #if VERBOSE:\n",
    "            #    print(\"- - Checking from \", start, \"to\", end)\n",
    "            #    pass\n",
    "            \n",
    "            # Skip the very first check, ie when end == start\n",
    "            # Not sure that this is the best way to do it, but the algorithm in Marchetti2017 says so.\n",
    "            if end != datablock['start_idx']:\n",
    "                \n",
    "                # End reached the last bit of the datablock. Close it.\n",
    "                if end == datablock['end_idx']:\n",
    "                    if VERBOSE:\n",
    "                        #DEBUG ONLY\n",
    "                        #print(\"- - End of block\")\n",
    "                        print(\"\\t  DATABLOCK\", {'start_idx': start, 'end_idx': datablock['end_idx']})\n",
    "                    remaining_blocks.append({'start_idx': start, 'end_idx': datablock['end_idx']})\n",
    "                    \n",
    "                # There is a change of magnitude. Close the block.\n",
    "                elif m[end] < m[oldindex]: \n",
    "                    if VERBOSE:\n",
    "                        #DEBUG ONLY\n",
    "                        #print(\"- - Change of magnitude\")\n",
    "                        print(\"\\t  DATABLOCK\", {'start_idx': start, 'end_idx': end-1})\n",
    "                        \n",
    "                    remaining_blocks.append({'start_idx': start, 'end_idx': end-1})\n",
    "                    start = end\n",
    "                \n",
    "                #DEBUG ONLY    \n",
    "                #else:\n",
    "                #    if VERBOSE:\n",
    "                #        print(\"- - Not end and not change of magnitude. go on\")\n",
    "                #        pass\n",
    "            \n",
    "            #DEBUG ONLY\n",
    "            #else:\n",
    "            #    if VERBOSE:\n",
    "            #        print(\"- - end == start SKIP\")\n",
    "            #        pass\n",
    "                    \n",
    "            oldindex = end\n",
    "            \n",
    "    # Returns the recognized values so far, and the remaining blocks to be analysed.\n",
    "    return processed_datablocks, remaining_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes the remaining blocks and recognise their type ([Marchetti2017](http://dx.doi.org/10.1109/TIFS.2018.2870826)).\n",
    "\n",
    "The intended return of the method consists of variable:\n",
    " - the `processed_datablocks` variable that includes the final variable split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_remaining_datablocks(uid, can, processed_datablocks, remaining_blocks, b, m):\n",
    "    \n",
    "    # 1% epsilon\n",
    "    EPSILON = 0.01\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(\"- Checking DATABLOCKS\")\n",
    "        \n",
    "    for block in remaining_blocks:\n",
    "        #pb.Comment = block\n",
    "        if VERBOSE:\n",
    "            print(\"\\t\" + str(block))\n",
    "        \n",
    "        # Check if it is a COUNTER\n",
    "        if block['start_idx'] != block['end_idx']:\n",
    "            \n",
    "            # magnitude of the least significant bit = 0 <-> bitiflip = 1\n",
    "            if b[block['end_idx']] == 1: \n",
    "                init = block['end_idx'] - 1\n",
    "                end_idx = block['end_idx']\n",
    "                while init >= block['start_idx']:\n",
    "                    # the bitflip rate should double at each step from the most \n",
    "                    # significant bit to the least significant\n",
    "                    if math.isclose(2*b[init], b[init+1], rel_tol=EPSILON):\n",
    "                        init = init - 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # Save this block as counter\n",
    "                if VERBOSE:\n",
    "                    print(\"\\t  COUNTER\", {'start_idx': init + 1, 'end_idx': end_idx})\n",
    "                    \n",
    "                processed_datablocks['counter'].append({'start_idx': init + 1, 'end_idx': end_idx})\n",
    "\n",
    "                block['end_idx'] = init\n",
    "            else:\n",
    "                if VERBOSE:\n",
    "                    print(\"\\t  Not a COUNTER\")\n",
    "        else:\n",
    "            if VERBOSE:\n",
    "                print(\"\\t  BINARY\", block)\n",
    "            processed_datablocks['binary'].append(block)\n",
    "        # END COUNTER\n",
    "        \n",
    "        # Discard all the blocks that have negative length after counter analysis\n",
    "        start_idx = block['start_idx']\n",
    "        end_idx = block['end_idx']\n",
    "        if start_idx < block['end_idx']:  \n",
    "            \n",
    "            out = False\n",
    "            for i in range(start_idx, end_idx):\n",
    "                exit = False\n",
    "                idx = b[i:end_idx + 1]\n",
    "                if VERBOSE:\n",
    "                    msg = \"\\t  Checking b[\"+str(i)+\" : \"+str(end_idx)+\"+1]\"\n",
    "                \n",
    "                # Check if the values in the datablock fit a normal distribution\n",
    "                # centered in 0.5 (so 0.5 ± std)\n",
    "                mean = statistics.mean(idx)\n",
    "                std = statistics.stdev(idx)\n",
    "                if (0.5 - std <= mean) and (mean <= 0.5 + std):\n",
    "                    for j in range(i, int(b[1] + 1)):\n",
    "                        # Beware, in READ algorithm this condition was different\n",
    "                        # but we are using floor instead of ceiling, so at the end \n",
    "                        # there are no changes\n",
    "                        \n",
    "                        if m[j] < -1: \n",
    "                            exit = True\n",
    "                    if exit is True:\n",
    "                        break\n",
    "                    else:\n",
    "                        if VERBOSE:\n",
    "                            print(\"\\t  CRC\", {'start_idx': i, 'end_idx': end_idx})\n",
    "                        processed_datablocks['crc'].append({'start_idx': i, 'end_idx': end_idx})\n",
    "                        end_idx = i - 1\n",
    "                        out = True\n",
    "                        if start_idx <= end_idx:\n",
    "                            if VERBOSE:\n",
    "                                print(\"\\t  Reinject block\", {'start_idx': start_idx, 'end_idx': end_idx})\n",
    "                            remaining_blocks.append({'start_idx': start_idx, 'end_idx': end_idx})\n",
    "                else:\n",
    "                    if VERBOSE:\n",
    "                        print(msg + \" - Not a CRC\") \n",
    "                if out:\n",
    "                    break\n",
    "            if not out:\n",
    "                size = block['end_idx'] - start_idx\n",
    "                if size <= 4:\n",
    "                    blocktype = \"nibble\"\n",
    "                elif size <=8:\n",
    "                    blocktype = \"byte\"\n",
    "                elif size <=16:\n",
    "                    blocktype = \"halfword\"\n",
    "                else:\n",
    "                    blocktype = \"word\"\n",
    "                        \n",
    "                if VERBOSE:\n",
    "                    print(\"\\t  \" + blocktype.upper(), {'start_idx': start_idx, 'end_idx': block['end_idx']})\n",
    "                processed_datablocks[blocktype].append({'start_idx': start_idx, 'end_idx': block['end_idx']})\n",
    "        else:\n",
    "            if VERBOSE:\n",
    "                print(\"\\t  SKIP (block has now negative size)\")    \n",
    "    return processed_datablocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the analysis and extract all the variables from the datablocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_trace(uid, can, bitflips_grp, magnitudes_grp):\n",
    "    current_raw_datablocks = raw_datablocks[(uid, can)]\n",
    "    \n",
    "    current_bitflips_grp = bitflips_grp.get_group((uid, can))\n",
    "    current_bitflips_grp.reset_index(inplace=True, drop=True)\n",
    "    current_bitflips_grp = list(current_bitflips_grp.loc[0,:])\n",
    "    del current_bitflips_grp[0]\n",
    "    del current_bitflips_grp[0]\n",
    "    \n",
    "    current_magnitudes_grp = magnitudes_grp.get_group((uid, can))\n",
    "    current_magnitudes_grp.reset_index(inplace=True, drop=True)\n",
    "    current_magnitudes_grp = list(current_magnitudes_grp.loc[0,:])\n",
    "    del current_magnitudes_grp[0]\n",
    "    del current_magnitudes_grp[0]\n",
    "    \n",
    "    processed_datablocks, remaining_blocks = process_raw_datablocks(uid, can,\n",
    "                                                                    current_raw_datablocks, \n",
    "                                                                    current_bitflips_grp, \n",
    "                                                                    current_magnitudes_grp)\n",
    "    processed_datablocks = process_remaining_datablocks(uid, can,\n",
    "                                                        processed_datablocks, \n",
    "                                                        remaining_blocks, \n",
    "                                                        current_bitflips_grp,\n",
    "                                                        current_magnitudes_grp)\n",
    "    return processed_datablocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "mb = progress_bar(raw_datablocks)\n",
    "for key in mb:\n",
    "    mb.comment = key\n",
    "    if VERBOSE:\n",
    "        print(key, \"-----------------------------\")\n",
    "    \n",
    "    # Finally save the resulted datablocks\n",
    "    datablocks[key] = process_trace(key[0], key[1], bitflips_grp, magnitudes_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the intermediate result as Pickle file to speedup future analysis\n",
    "#with open(DATA_DIR + 'raw.datablocks.pkl', 'wb') as handle:\n",
    "#    pickle.dump(datablocks, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract values from DATA and BINARY Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function converts a ID/CANline trace to a collection of variables. \n",
    "In the associated paper, this is defined as `unified` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(DATA_DIR + 'raw.datablocks.pkl', 'rb') as handle:\n",
    "    datablocks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def extract_variables_values(uid, can, df, datablocks, datatypes = ['binary', 'nibble', 'byte', 'halfword','word']):\n",
    "    rows = []\n",
    "    for row in df.itertuples():\n",
    "        bits = row.binary\n",
    "        \n",
    "        for datatype in datatypes:\n",
    "            # Data\n",
    "            n = 0\n",
    "            for datablock in datablocks[datatype]:\n",
    "                variable = bits[datablock['start_idx']:datablock['end_idx']+1]\n",
    "                rows.append({\n",
    "                    'time': row.time,\n",
    "                    'id': uid,\n",
    "                    'can': can,\n",
    "                    'datatype': datatype,\n",
    "                    'variable' : datatype[0:2].upper() + \"_\" + str(n),\n",
    "                    'value' : int(variable, base=2)\n",
    "                })\n",
    "                n += 1\n",
    "            \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grps = df.groupby(by=[\"id\",\"can\"])\n",
    "uid, can = next(iter(grps.groups))\n",
    "\n",
    "res = extract_variables_values(uid, can, grps.get_group((uid, can)), datablocks[(uid, can)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pd.DataFrame(res).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the variables foreach trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unified = []\n",
    "pb = progress_bar(df.groupby(by=[\"id\",\"can\"]))\n",
    "for (uid, can), group in pb:\n",
    "    pb.comment = uid + \" - \" + can\n",
    "    variables = extract_variables_values(uid, can, group, datablocks[(uid, can)])\n",
    "    unified.extend(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unified = pd.DataFrame(unified)\n",
    "\n",
    "display(unified.info())\n",
    "display(unified.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(DATA_DIR + 'unified.pkl', 'wb') as handle:\n",
    "    pickle.dump(unified, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#unified.to_csv(DATA_DIR + 'unified.csv', sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export figures\n",
    "\n",
    "### Packet count per ID and CANline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "def packet_count_per_id_and_canline(df, show=True, save=True):  \n",
    "    grp = df.groupby(['can','id']).agg(['count'])['binary'].reset_index()\n",
    "    grp['log'] = np.log10(grp['count'])\n",
    "    grp.head()\n",
    "\n",
    "    var = 'log'\n",
    "    palettes = {\n",
    "        'can0': \"Blues_r\",\n",
    "        'can1': 'Greens_r'\n",
    "    }\n",
    "\n",
    "    for can in grp.can.unique():\n",
    "        sns.set(font_scale=1.2)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.set_style({'font.family':'monospace'})\n",
    "\n",
    "        sub = grp[grp.can==can].copy()\n",
    "        sub.sort_values(by=var, ascending=False, inplace=True)\n",
    "        sub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        fig = plt.figure(figsize=(15,5))\n",
    "        ax = sns.barplot(data=sub, x='id', y=var, order=sub['id'], palette=palettes[can])\n",
    "        plt.xticks(rotation='vertical')\n",
    "        plt.ylim(0,max(sub[var])+1)\n",
    "        #[x.set_va('top') for x in ax.get_xticklabels()]\n",
    "        if var == 'log':\n",
    "            t = ['%.0E' % (10**y) for y in ax.get_yticks()]\n",
    "            ax.set_yticklabels(t)\n",
    "            ax.set(xlabel=\"ID CODE\", ylabel=\"Number of packets (log scale)\")\n",
    "        else:\n",
    "            ax.set(xlabel=\"ID CODE\", ylabel=\"Number of packets\")\n",
    "\n",
    "        #plt.legend(loc=\"upper right\", title=\"CAN line\")\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            plt.savefig(GRAPHICS_DIR + var + \"-\" + can + \".\" + FIG_EXTENSION)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "#packet_count_per_id_and_canline(df, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrarrival packet time distributions and boxplots\n",
    "\n",
    "#### Individual distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def formatter(value, tick_number):\n",
    "    return \"{:.2f}\".format(value).rjust(8)\n",
    "\n",
    "def timedeltas(data, uid, xlim_min, xlim_max, show=True, save=True):\n",
    "    \n",
    "    \n",
    "    grps = data.groupby(by='can')\n",
    "    fig = plt.figure(figsize=(15, 1 + 2*len(grps)),\n",
    "                     constrained_layout=True,)\n",
    "    \n",
    "    fig.suptitle(\"Identifier: \" + uid, y=1, x=0, ha='left', fontsize='large', fontweight='bold')\n",
    "    \n",
    "    spec = gridspec.GridSpec(ncols=2, \n",
    "                             nrows=len(grps), \n",
    "                             figure=fig,\n",
    "                             width_ratios = [10,2],\n",
    "                             hspace=1,\n",
    "                             wspace=0\n",
    "                            )\n",
    "    \n",
    "    #fig.suptitle('Interarrival for ID ' + str(uid), y=1.05)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_style({'font.family':'monospace'})\n",
    "    \n",
    "    palettes = {\n",
    "        \"can0\": '#4C72B0',\n",
    "        \"can1\": '#C44E52'\n",
    "    }\n",
    "\n",
    "    ax_idx = 0\n",
    "    for can, d in grps:    \n",
    "        if (len(d) < 1):\n",
    "            continue\n",
    "        \n",
    "        lax = fig.add_subplot(spec[ax_idx, 0])\n",
    "        lax = sns.scatterplot(data=d, x=d.time, y=d.timedelta_ms, ax=lax, color=palettes[can])\n",
    "        lax.xaxis.grid(False)\n",
    "        lax.set_ylabel('Timedelta in ms')\n",
    "        lax.set_xlabel('Packet arrival time for (' + uid + ', ' + can + ')')\n",
    "        lax.set_xlim(left=xlim_min, right=xlim_max)\n",
    " \n",
    "        lax.yaxis.set_major_formatter(plt.FuncFormatter(formatter))\n",
    "    \n",
    "        rax = fig.add_subplot(spec[ax_idx, 1], sharey=lax)\n",
    "        rax = sns.boxenplot(data=d, \n",
    "                            y=d.timedelta_ms, \n",
    "                            color=palettes[can],\n",
    "                            ax=rax)\n",
    "        rax.set(xlabel=can, ylabel=None, title=\"Boxplots\")\n",
    "        rax.yaxis.set_major_formatter(plt.FuncFormatter(formatter))\n",
    "        \n",
    "        ax_idx += 1\n",
    "    #plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        try:\n",
    "            os.makedirs(GRAPHICS_DIR + \"timedeltas/\")\n",
    "        except FileExistsError:\n",
    "            # directory already exists\n",
    "            pass\n",
    "        plt.savefig(GRAPHICS_DIR + \"timedeltas/timedelta-\" + uid + \".\" + FIG_EXTENSION, bbox_inches = \"tight\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.close()\n",
    "\n",
    "#test_id = next(iter(df.id.unique()))\n",
    "#timedeltas(uid=test_id, show=True, save=False,\n",
    "#           xlim_min = xlim_min,\n",
    "#           xlim_max = xlim_max,\n",
    "#           data=df.groupby('id').get_group(test_id)#.sample(1000)\n",
    "#          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the interarrival time for each UID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid, group in progress_bar(df.groupby(by='id')):\n",
    "    try:\n",
    "        timedeltas(data=group, \n",
    "                   uid=uid, \n",
    "                   xlim_min=xlim_min,\n",
    "                   xlim_max=xlim_max,\n",
    "                   show=False, \n",
    "                   save=True)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception with \", uid, ex)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall view of vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def timedeltaboxes(data, \n",
    "                   show=True, \n",
    "                   save=False, \n",
    "                   log = False, \n",
    "                   fliers = [True, True]\n",
    "                  ):\n",
    "    import matplotlib as mpl\n",
    "    palettes = {\n",
    "        \"can0\": '#4C72B0',\n",
    "        \"can1\": '#C44E52'\n",
    "    }\n",
    "       \n",
    "    fig = plt.figure(figsize=(15,3), constrained_layout=True)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_style({'font.family':'monospace'})\n",
    "    \n",
    "    spec = gridspec.GridSpec(ncols=2, \n",
    "                             nrows=1, \n",
    "                             figure=fig,\n",
    "                             width_ratios = [10,2],\n",
    "                             hspace=1,\n",
    "                             wspace=0\n",
    "                            )\n",
    "    \n",
    "    lax = fig.add_subplot(spec[0, 0])\n",
    "    lax = sns.boxplot(data=data, \n",
    "                      x='id', \n",
    "                      hue='can', \n",
    "                      y='timedelta_ms', \n",
    "                      ax=lax,\n",
    "                      palette=palettes, \n",
    "                      showfliers=fliers[0]\n",
    "                     )\n",
    "    plt.xticks(rotation='vertical')\n",
    "    lax.set_ylabel('Timedelta in ms')\n",
    "    lax.set_xlabel('IDs')\n",
    "\n",
    "    plt.legend(loc=\"best\", ncol=2, title=\"CAN line\")\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "    rax = fig.add_subplot(spec[0, 1], sharey=lax)\n",
    "    rax = sns.boxenplot(data=data, \n",
    "                        x='can', \n",
    "                        y='timedelta_ms', \n",
    "                        palette=palettes,\n",
    "                        ax=rax,\n",
    "                        linewidth=0,\n",
    "                        #showfliers=fliers[2]\n",
    "                       )\n",
    "    rax.set_ylabel('')\n",
    "    rax.set_xlabel('CAN lines')\n",
    "\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "        rax.get_yaxis().set_major_locator(mpl.ticker.LogLocator())\n",
    "        rax.get_yaxis().set_major_formatter(mpl.ticker.LogFormatter())\n",
    "        #rax.get_yaxis().set_minor_locator(mpl.ticker.LogLocator())\n",
    "        #rax.get_yaxis().set_minor_formatter(mpl.ticker.LogFormatter())\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(GRAPHICS_DIR + \"interarrivalpackettime\" + (\"-log\" if log else \"\") + \n",
    "                    \".\" + FIG_EXTENSION, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "#timedeltaboxes(show=True, \n",
    "#               save=False, \n",
    "#               log=False,\n",
    "#               data=df#.head(1000)\n",
    "#               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the actual figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltaboxes(show=True, \n",
    "               save=True, \n",
    "               fliers = [False, False],\n",
    "               log=True,\n",
    "               data=df#.sample(10000)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitflips heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitflips_heatmap(results, \n",
    "                     show=True, \n",
    "                     save=False, save_filename_extra=\"\",\n",
    "                     row_height = 0.2, \n",
    "                     palette = sns.color_palette('gray_r', 5),\n",
    "                     ylim_min = 0, ylim_max = 1, \n",
    "                     hlines=None, vlines=None):\n",
    "    \n",
    "    labels = results.id + ' ' + results.can\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,row_height*len(results.index),))\n",
    "    #fig = plt.figure(figsize=(15,3))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_style({'font.family':'monospace'})\n",
    "    sns.set_context(\"notebook\", font_scale=0.8, rc={\"lines.linewidth\": 2.5})\n",
    "    ax = sns.heatmap(data=results[results.select_dtypes('number').columns], \n",
    "                     yticklabels=labels,\n",
    "                     linewidths=.2, \n",
    "                     vmin=ylim_min, \n",
    "                     vmax=ylim_max, \n",
    "                     cmap=palette\n",
    "                    )\n",
    "    if vlines:\n",
    "        for line in vlines:\n",
    "            ax.vlines(x=line['x'], \n",
    "                      ymin=line['ymin'], \n",
    "                      ymax=line['ymax'], \n",
    "                      colors=line['colors'], \n",
    "                      linewidth = 2,\n",
    "                      #linestyles='dotted', \n",
    "                      label=line.get('label'))\n",
    "    if hlines:\n",
    "        for line in hlines:\n",
    "            ax.hlines(y=line['y'], \n",
    "                      xmin=line['xmin'], \n",
    "                      xmax=line['xmax'], \n",
    "                      colors=line['colors'], \n",
    "                      linewidth = 2,\n",
    "                      #linestyles='dotted', \n",
    "                      label=line.get('label'))\n",
    "    \n",
    "    ax.xlabel = \"Bit position in the data field\"\n",
    "    ax.ylabel = \"ID and CAN line\"\n",
    "    plt.yticks(rotation=0) \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    \n",
    "    if vlines or hlines:\n",
    "        plt.legend(loc='lower center', \n",
    "                   ncol=7,\n",
    "                   bbox_to_anchor=(0.5, 1),\n",
    "                   title='Var. Type'\n",
    "                  )\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        filename = GRAPHICS_DIR + \"bitflips-heatmap\" + save_filename_extra + (\"-annotated\" if hlines or vlines else \"\") + \".\" + FIG_EXTENSION\n",
    "        plt.savefig(filename, bbox_inches = \"tight\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "#bitflips_heatmap(bitflips.sample(10), \n",
    "#                 show=True, \n",
    "#                 save=False,\n",
    "#                 row_height = 0.2, \n",
    "#                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the heatmap with colored boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variable_boxes(datablocks):\n",
    "    boxes_palette = {\n",
    "        'binary': ['#DB5E56'],\n",
    "        'counter': ['#91DB56'],\n",
    "        'crc': ['#B15928'],\n",
    "        'nibble': ['#DBC256'],\n",
    "        'byte': ['#0F8935'],\n",
    "        'halfword': ['#566FDB'],\n",
    "        'word': ['#DB56B2'],\n",
    "    }\n",
    "    labels = {\n",
    "        'binary': False,\n",
    "        'counter': False,\n",
    "        'crc': False,\n",
    "        'nibble': False,\n",
    "        'byte': False,\n",
    "        'halfword': False,\n",
    "        'word': False,\n",
    "    }\n",
    "    \n",
    "    boxes = [\n",
    "        # Prototype:\n",
    "        #{'x': 0,   'y': 0,   'w': 3,   't': 'short'},\n",
    "    ]\n",
    "    \n",
    "    idx = 0\n",
    "    for key, boxes_dict in datablocks.items():\n",
    "        for var_type, var_type_boxes in boxes_dict.items():\n",
    "            for box in var_type_boxes:\n",
    "                boxes.append(\n",
    "                    {'x': box['start_idx'],\n",
    "                     'y': idx,\n",
    "                     'w': 1 + box['end_idx'] - box['start_idx'],\n",
    "                     't': var_type}\n",
    "                )\n",
    "        idx += 1\n",
    "\n",
    "    vlines = []\n",
    "    hlines = []\n",
    "\n",
    "    for box in boxes:\n",
    "        vlines.append({'x': box['x'],\n",
    "                       'ymin': box['y'],\n",
    "                       'ymax': box['y'] +1,\n",
    "                       'colors': boxes_palette[box['t']], \n",
    "                       'label': box['t'].capitalize() if not labels[box['t']] else None,\n",
    "                      })\n",
    "        labels[box['t']] = True\n",
    "\n",
    "        vlines.append({'x': box['x'] + box['w'],\n",
    "                       'ymin': box['y'],\n",
    "                       'ymax': box['y'] +1,\n",
    "                       'colors': boxes_palette[box['t']], \n",
    "                       'label': box['t'].capitalize() if not labels[box['t']] else None,\n",
    "                      })\n",
    "\n",
    "        hlines.append({'y': box['y'],\n",
    "                       'xmin': box['x'],\n",
    "                       'xmax': box['x'] + box['w'],\n",
    "                       'colors': boxes_palette[box['t']], \n",
    "                       'label': box['t'].capitalize() if not labels[box['t']] else None,\n",
    "                      })\n",
    "        hlines.append({'y': box['y'] + 1,\n",
    "                       'xmin': box['x'],\n",
    "                       'xmax': box['x'] + box['w'],\n",
    "                       'colors': boxes_palette[box['t']], \n",
    "                       'label': box['t'].capitalize() if not labels[box['t']] else None,\n",
    "                      })\n",
    "    \n",
    "    return vlines, hlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIlter by ID and create the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "magnitudes = pd.read_pickle(DATA_DIR + \"raw.magnitudes.pkl\")\n",
    "with open(DATA_DIR + 'raw.datablocks.pkl', 'rb') as handle:\n",
    "    datablocks = pickle.load(handle)\n",
    "\n",
    "ids = list(df.id.unique())\n",
    "filtered = magnitudes[magnitudes.id.isin(ids).values]\n",
    "#filtered = magnitudes\n",
    "display(filtered.head())\n",
    "\n",
    "tmp = filtered.select_dtypes(include=np.number)\n",
    "min_scale = int(np.around(np.abs(np.nanmin(tmp[tmp != -np.inf]))))\n",
    "x_loc = 0 # replace this with the X coordinate of a -Inf value\n",
    "y_loc = 0 # replace this with the Y coordinate of a -Inf value\n",
    "filtered = filtered.replace(filtered.loc[x_loc,y_loc], -1 -min_scale) #Replace the infinite values with min - 1\n",
    "display(filtered.head())\n",
    "grps = filtered.groupby(by=['id','can'])\n",
    "\n",
    "filtered_datablocks = { key: datablocks[key] for key in grps.groups.keys() }\n",
    "#display(filtered_datablocks)\n",
    "\n",
    "vlines, hlines = create_variable_boxes(filtered_datablocks)\n",
    "\n",
    "ROW_HEIGHT = 0.2\n",
    "\n",
    "bitflips_heatmap(filtered, \n",
    "                 show=True, \n",
    "                 save=True, save_filename_extra='-magnitude',\n",
    "                 row_height = ROW_HEIGHT, \n",
    "                 palette = [\"#E9EBEB\",\"#D0D2D2\",\"#B6B8B8\",\"#838585\",\"#505252\"], #Light to dark\n",
    "                 #palette = [\"#505252\",\"#838585\",\"#B6B8B8\",\"#D0D2D2\",\"#E9EBEB\"], # Dark to light\n",
    "                 ylim_min = -5, ylim_max = 0, \n",
    "                 #hlines=hlines, vlines=vlines\n",
    "                )\n",
    "\n",
    "bitflips_heatmap(filtered, \n",
    "                 show=True, \n",
    "                 save=True, save_filename_extra='-magnitude',\n",
    "                 row_height = ROW_HEIGHT, \n",
    "                 palette = [\"#E9EBEB\",\"#D0D2D2\",\"#B6B8B8\",\"#838585\",\"#505252\"], #Light to dark\n",
    "                 #palette = [\"#505252\",\"#838585\",\"#B6B8B8\",\"#D0D2D2\",\"#E9EBEB\"], # Dark to light\n",
    "                 ylim_min = -5, ylim_max = 0, \n",
    "                 hlines=hlines, vlines=vlines\n",
    "                )\n",
    "\n",
    "\n",
    "bitflips = pd.read_pickle(DATA_DIR + \"raw.bitflips.pkl\") \n",
    "filtered = bitflips[bitflips.id.isin(ids).values]\n",
    "bitflips_heatmap(filtered, \n",
    "                 show=True, \n",
    "                 save=True, save_filename_extra='-percentage',\n",
    "                 row_height = ROW_HEIGHT, \n",
    "                 palette = [\"#E9EBEB\",\"#D0D2D2\",\"#B6B8B8\",\"#838585\",\"#505252\"], #Light to dark\n",
    "                 #palette = [\"#505252\",\"#838585\",\"#B6B8B8\",\"#D0D2D2\",\"#E9EBEB\"], # Dark to light\n",
    "                 #ylim_min = -5, ylim_max = 0, \n",
    "                 #hlines=hlines, vlines=vlines\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitflips = pd.read_pickle(DATA_DIR + \"raw.bitflips.pkl\")\n",
    "#magnitudes = pd.read_pickle(DATA_DIR + \"raw.magnitudes.pkl\")\n",
    "\n",
    "vlines, hlines = create_variable_boxes(datablocks)\n",
    "\n",
    "bitflips_heatmap(bitflips, \n",
    "                 show=True, \n",
    "                 save=False, #save_filename_extra='-' + ('-'.join(magnitudes.id.unique())),\n",
    "                 row_height = 0.2, \n",
    "                 #palette = sns.color_palette('gray_r', 5),\n",
    "                 palette = [\"#E9EBEB\",\"#D0D2D2\",\"#B6B8B8\",\"#838585\",\"#505252\"], #Light to dark\n",
    "                 #palette = [\"#505252\",\"#838585\",\"#B6B8B8\",\"#D0D2D2\",\"#E9EBEB\"], # Dark to light\n",
    "                 ylim_min = 0, ylim_max = 0.1, \n",
    "                 hlines=hlines, vlines=vlines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
